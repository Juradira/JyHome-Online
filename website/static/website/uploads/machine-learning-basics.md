# Machine Learning Basics

Find patterns that generalize to new data.
Determin hyperparameters to the learning algorithm itself.

*Machine learning is essentially a form of applied statstics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions.*


Two central qpproaches to statistics:
- frequentist estimators
- bayesian inference

Most machine learning algorithms can be divided into the categories of:
- supervised learning
- unsupervised learning

Usually, we need to combine various algorithm components such as an optimization algorithm, a cost function, a model and a dataset to build a machine learning algorithm.


## Learning Algorithms
Definition:
> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

#### Task, T
Learning is our means of attaining the ability to perform the task. Machine learning tasks are usually described in terms of how the machine learning system should process an example. Some of the most common machine learning tasks include the following:
- Classification
- Classification with missing inputs
- Regression
- Transcription
- Machine translation
- Structured output
- Anomaly detection
- Synthesis and sampling
- Imputation of missing values
- Denoising
- Density estimations or probability mass function estimation

#### Performance Measure, P
In order to evaluate the abilites of a machine learning algorithm, we must design a quantitative measure of its performance. Usually this performance measure P is specific to the task T being carried out by the system.

For some tasks such as classification, we often measure the **accuracy** of the model. Also, we can use the **error rate**.

For tasks such as density estimation, we must use a different performance metric that gives the model a continuous-valued score for each example. The most common approach is to report the average log-probability the model assigns to some examples.

We use test set of data that is separate from the data used for training the machine learning system to evaluate the performance measure.

#### Experience, E
Many learning algorithms can be understood as being allowed to experience an entire dataset.

Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. Supervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target.

Roughly speaking, unsupervised learning involes observing several examples of a random vector **$x$**, and attempting to implicitly or explicitly learn the probability distribution **$p(x)$**, or some interesting properties of that distribution, while supervised learning involes observing several examples of a random vector **$x$** and an associated value or vector **$y$**, and learning to predict **$y$** from **$x$**, usually by estimating **$p(y|x)$**. But they are not formally defined terms, the lines between them are often blurred.

There are many other machine learning algorithms such as semi-supervised learning, multi-instance learning, reinforement learning ...

One common way of describing a dataset is with a design matrix. A design matrix is a matrix containing a different example in each row. Each column of the matrix corresponds to a different feature. Of course, to describe a dataset as a design matrix, it must be possible to describe each example as a vector, and each of these vectors must be the same size. This is not always possible.

## Capacity, Overfitting and Underfitting
The central challenge in machine learning is that we must perform well on *new, previously unseed* input - not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called *generalization*.

The train and test data are generated by a probability distribution over datasets called the *data generating process*. We often make a set of assumptions known collectively as the *i.i.d assumptions* These assumptions are that the examples in each dataset are *independent* from each other, and that the train set and test set are *identically distributed*, drawn from the same probability distribution as each other.

There are two central challenges in machine learning: **underfitting** and **overfitting**. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.

We can control whether a model is more likely to overfit or underfit by altering its **capacity**. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. One way to control the capacity of a learning algorithm is by choosing its **hypothesis space**, the set of functions that the learning algorithm is allowed to select as being the solution.

Capacity is not determined only by the choice of model.

**Occam's razor**: Among competing hypothese that explain known observations equally well, one should choose the "simplest" one.

**Vapnik-Chervonenkis dimension**: A method measures the capacity of a binary classifier. The VC dimension is defined as being the largest possible value of m for which there exists a training set of *m* different *x* points that the classifier can label arbitrarily.

**Non-Parametric Models**: Cause parametric models learn a function described by a parameter vector whose size if finite and fixed before any data is observed. Non-parametric models have no such limitation, such as *nearest neighbor regression*.

The error incurred by an oracle making predictions from the true distribution $p(x,y)$ is called the **Bayes Error**.

#### The No Free Lunch Theorem
Machine learning promises to find rules that are *probably* correct about *most* members of the set they concern. The *no free lunch theorem* states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unoberved points. Fortunately, these results hold only when we average over all possible data generating distributions. If we make assumptions about the kinds of probability distributions we cncounter in real-world applications, then we can design learning algorithms that perform well on these distributions.

It means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the "real world" that an AI agent experiences and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.

#### Regularization
For example, we modify the training criterion for linear regression to include *weight decay*, as following shows:
$$J(\omega)=MSE\_{train}+\lambda\omega^T\omega$$
where $\lambda$ is a value chosen ahead of time that controls the strength of our preference for smaller weights. The larger $\lambda$ forces the weights to become smaller.

Generally, we can regularize a model that learns a function $f(x;\theta)$ by adding a penalty called a *regularizer* to the cost function. In the case of weight decay, the regularizer is $\Omega(\omega)=\omega^T\omega$. There are many other regularizers are possible.

Expressing preferences for one function over another is a more general way of controlling a model's capacity than including or excluding members from the hypothesis space. In weight decay example, we expressed our preference for functions defined with smaller weights explicitly, via an extra term in the criterion we minimize. **Regularization is any modificaion wemake to a learning algorithm that is intended to reduce its generalizaion error but not its training error.**

## Hyperparameters and Validation Sets
Hyperparameters control the behavior of the learning algorithm. In the polynomial regression example above, there is a single hyperparameter: the degree of the polynomial, which acts as a capacity hyperparameter. The $\lambda$ value is another example of a hyperparameter. More frequently, we do not learn the hyperparameter because it is not appropriate to learn that hyperparameter on the training set. If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting.

We use a **validation set** of examples that the training algorithm does not observe.

Specifically, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters. The other subset is our validation set, used to estimate the generalization error during of after training, allowing for the hyperparameters to be updated accordingly. After all hyperparameter optimization is complete, the generalizaion error may be estimated using the test set.

#### Cross-validation
It will make it difficult to claim that which algorithm is better on the given task if the test set is small. When the dataset is too small, there are alternative procedures based on the idea of repeating the training and testing computation on different randomly chosen subsets or splits of the original dataset. The most commom of these if the **k-fold** cross-validation procefure.

## Estimators, Bias and Variance
Foundational concepts such as parameter estimation, bias and variance are useful to formally characterize notions of generalization, underfitting and overfitting.

**Point Estimation**: Point estimation is the attempt to provide the single "best" prediction of some quanitity of interest. Let {$x^{(1)},...,x^{(m)}$} be a set of m independent and identically distributed data points. A point estimator os statistic is any function of the data:
$$\hat{\theta}\_m=g(x^{(1)},...,x^{(m)})$$
While alomst any function thus qualifies as an estimator, a good estimator is a function whose output is close to the true underlying $\theta$ that generated the training data.

**Function Estimation**: If we want to predict a variable $y$ given an input vector $x$. We assume that there is a function $f(x)$ that describes the approximate relationship between $y$ and $x$. For example, we may assume that $y=f(x)+\epsilon$, where $\epsilon$ stands for the part of $y$ that is not predictable from $x$.

#### Bias
The bias of an estimator is defined as:
$$bias(\hat{\theta\_m})=\mathbb{E}(\hat{\theta})-\theta$$
An  estimator $\hat{\theta}\_m$ is said to be *asymptotically unbiased* if $\lim\limits\_{m\to\infty}bias(\hat{\theta}\_m)=0$, while implied that $\lim\limits\_{m\to\infty}\mathbb{E(\hat{\theta}\_m)}=\theta$.

Some estimators are unbiased, such as *Bernoulli Distribution*, *Gaussian Distribution Estimator of Mean ...*

#### Variance and Standard Error
The variance or the standard error of an estimator provides a measure of how we would expect the estimate we compute from data to vary as we independently resample the dataset from the underlying data generating process. Just as we might like an estimator to exhibit low bias we would also like it to have relatively low variance.

#### Trading off Bias and Variance to Minimize Mean Squared Error
*Mean Squared Error:*
$$MSE=\mathbb{E}[(\hat{\theta}\_m-\theta)^2]=Bias(\hat{\theta}\_m)^2+Var(\hat{\theta}\_m)$$
The MSE measures the overall expected deviation - in a squared error sense - between the estimator and the true value of the parameter $\theta$.

The relationship between bias and variance is tightly linked to the machine learning concepts of capacity, underfitting and overfitting. **In the case where generalization error is measured by the MSE(where bias and variance are meaningful components of generalization error), increasing capacity tends to increase variance and decrease bias.**

#### Consistency
In particular, we usually wish that, as the number of data points *m* in our dataset increases, our point estimates converge to the true value of the corresponding patameters. More formlly, we could like that
$$\lim\limits\_{m\to\infty}\hat{\theta}\_m\xrightarrow{p}\theta$$
For any $\epsilon>0$, $P(|\hat{\theta}\_m|-\theta)>\epsilon\to0$ as $m\to\infty$. The symbol $\xrightarrow{p}$ means that the convergence is in probability. This is known as **consistency**.

## Maximum Likelihood Estimation
Consider a set of $m$ examples $\mathbb{X}=\{x^{(1)},...,x^{(m)}\}$ drawn independently from the true but unknown data generating distribution $p\_{data}(x)$. The maximum likelihood estimator for $\theta$ is then defined as
$$\theta\_{ML}=\underset{\theta}{\arg\max}p\_{model}(\mathbb{X};\theta)=\underset{\theta}{\arg\max}\prod\_{i=1}^n p\_{model}(x^{(i)};\theta)$$

We tranform a product into a sum:
$$\theta\_{ML}=\underset{\theta}{\arg\max}\sum\_{i=1}^m \log p\_{model}(\mathbb{X};\theta)$$

Also, we can divide by *m* to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution $\hat{p}\_{data}$ defined by the training data:
$$\theta\_{ML}=\underset{\theta}{\arg\max}\mathbb{E}\_{x\sim\hat{p}\_{data}} \log p\_{model}(\mathbb{X};\theta)$$

Anthor way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution $\hat{p}\_{data}$ defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. The KL divergence is given by
$$D\_{KL}(\hat{p}\_{data}||p\_{model})=\mathbb{E}\_{x\sim\hat{p}\_{data}}[\log\hat{p}\_{data}(x)-\log p\_{model}(x)]$$

The term on the left is a function only of the data generating process, not the model. This means we need only minimize
$$-\mathbb{E}\_{x\sim\hat{p}\_{data}}[\log p\_{model}(x)]$$
which if of course the same as the maximization above.

Minimizing the KL divergence corresponds exactly to minimizing the cross-entropy between the distribution. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the model. For example, mean squared error is the cross-entropy between the empirical distribution and a Gaussian model.

## Bayesian Statstics
So far we have discussed *frequentist statistic* and approaches based on estimating a single value of $\theta$, then making all predictions thereafter based on that one estimate. Another approach is to consider all possible values of $\theta$ when making a prediction. The latter is the domain of **Bayesian Statstics**.

We represent our knowledge of $\theta$ using the **prior probability distribution, $p(\theta)$**. For example, one might assume **a prior** that $\theta$ lies in some finite range or volume, with a uniform distribution.

Now consider that we have a set of data samples $\{x^{(1)},...,x^{(m)}\}$. We can recover the effect of data on our belief about $\theta$ by combining the data likelihood $p(x^{(1)},...,x^{(m)}|\theta)$ with the prior via Bayes' rule:
$$p(\theta|x^{(1)},...,x^{(m)})=\frac{p(x^{(1)},...,x^{(m)}|\theta)p(\theta)}{p(x^{(1)},...,x^{(m)})}$$

Relative to maximum likelihood estimation, Bayesian estimation offers two important differences. First, unlike the maximum likelihood approach that makes predictions using a point estimate of $\theta$, the Bayesian approach is to make predictions using a full distribution over $\theta$.

The second important difference between the Bayesian approach to estimation and the maximum likelihood approach is due to the contribution of the Bayesian prior distribution. The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred *a prior*.


## Supervised Learning Algorithms
#### Probabilistic Supervised Learning
We have already seen that linear regression corresponds to the family
$$p(y|x;\theta)=\mathcal{N}(y;\theta^Tx,I)$$
We can generalize linear regression to the classification scenario by defining a different family of probability distributions. We can use the logistic sigmoid function to squash the output of the linear function into the interval $(0,1)$ and interpret that value as a probability:
$$p(y=1|x;\theta)=\sigma(\theta^Tx)$$
This approach is known as **logistic regression**.

#### Support Vector Machines
Unlike logistic regression, the SVM does not provide probabilities, but onlu outputs a class identity.
One key innovation associated with support vector machines is the *kernel trick*. For example, it can be shown that the linear function used by the support vector machine can be re-written as
$$w^Tx+b=b+\sum\_{i=1}^{m}\alpha\_ix^Tx^{(i)}$$
where $x^{(i)}$ is a training example and $\alpha$ is a vector of coefficients. Rewriting the learning algorithm this way allows us to replace $x$ by the output of a given feature function $\phi(x)$ and the dot product with a function $k(x,x^{(i)})=\phi(x)\cdot\phi(x^{(i)})$ called a *kernel*.

After replacing dot products with kernel evaluations, we can make predictions using the function
$$f(x)=b+\sum\_i\alpha\_ik(x,x^{(i)})$$
This function is nonlinear with respect to $x$, but the relationship between $\phi(x)$ and $f(x)$ is linear. Also, the relationship between $\alpha$ and $f(x)$ is linear.

The kernel trick is powerful for two reasons. First, it allows us to learn models that are nonlinear as a function of $x$ using convex optimization techniques that are guaranteed to converge efficiently. Second, the kernel function $k$ often admits an implementation that is significantly more computational efficient than naively constructing two $\phi(x)$ vectors and explicitly taking their dot product.

## Unsupervised Learning Algorithms
There are three commom ways of defining a simpler representation:
- Lower Dimensional Representations
- Sparse Representations
- Independent Representations

#### Principal Components Analysis (PCA)
Having a $m\times n$-dimensional design martrix $X$, assume $\mathbb{E}[x]=0$. The unbiased sample convariance matrix is
$$Var[x]=\frac{1}{m-1}X^TX$$
PCA finds a representation $z=x^TW$ where $Var[z]$ is diagonal.
The principal components of a design matrix $X$ are given by the eigenvectors of $X^TX$. From this view,
$$X^TX=W\Lambda W^T$$

We can use SVD to exploit an alternative derivation of the principal components.
$$U^TU=I$$
$$X^TX=(U\Lambda W^T)^TU\Lambda W^T=W\Sigma^2W^T$$
So we have
$$Var[x]=\frac{1}{m-1}X^TX=\frac{1}{m-1}W\Sigma^2W^T$$
If we take $z=x^TW$, we can ensure that the convariance of $z$ is diagonal as required:
$$WW^T=I$$
$$Var[z]=\frac{1}{m-1}Z^TZ=\frac{1}{m-1}W^TX^TXW=\frac{1}{m-1}WW^T\Sigma^2WW^T=\frac{1}{m-1}\Sigma^2$$


## Stochastic Gradient Descent
Giving a cost function, such as the negative conditional log-likelihood, we can write as
$$J(\theta)=\frac{1}{m}\sum\_{i=1}^mL(x^{(i)},y^{(i)},\theta)$$
Gradient descent requires competing
$$\nabla\_\theta J(\theta)=\frac{1}{m-1}\sum\_{i=1}^m\nabla\_\theta L(x^{(i)}, y^{(i)}, \theta)$$
The computational clost of this operation is $O(m)$, As the training set size grows to billions of examples, the time to take a single gradient step becomes prohibitively long. To solve this problem, considering that the gradient is an expectation, we can use a minibatch $\mathbb{B}$ to calculate the gradient value and update the algorithm.
$$g=\frac{1}{m'}\nabla\_\theta \sum\_{i=1}^{m'}L(x^{(i)},y^{(i)},\theta)$$
$$\theta\gets\theta-\epsilon g$$
where $\epsilon$ is the learning rate.

## Building a Machine Learning Algorithm

- *a dataset*
- *a cost function*
- *an optimization procedure*
- *a model*

Most machine learning algorithms make use of recipes above. If a machine learning algorithm seems especially unique or hand-designed, it can usually be understood as using a sspecial-case optimizer. Recognizing that most machine learning algorithms can be described using this recipe helps to see the different algorithms as part of a taxonomy of methods for doing related tasks that work for similar reasons, rather than as a long list of algorithms that each have separate justifications.
